"""
Test HHV (German vinyl store) with ScrapingBot
"""
import asyncio
import httpx
from bs4 import BeautifulSoup
import re

USERNAME = "Vinilogy"
API_KEY = "4bdzoKp01ykZUwBpFU4OVQIJ8"

async def test_hhv():
    """Test HHV Records search"""
    
    print("=" * 70)
    print("Testing HHV Records (Germany)")
    print("=" * 70)
    
    # HHV search URL
    search_url = "https://www.hhv.de/en-ES/catalog/filter/search-S11?af=true&term=OK%20computer%20radiohead"
    
    endpoint = "http://api.scraping-bot.io/scrape/raw-html"
    
    # Try without premium first
    payload = {
        "url": search_url,
        "options": {
            "useChrome": False,
            "premiumProxy": False,
            "proxyCountry": "DE"  # German proxy
        }
    }
    
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json"
    }
    
    auth = (USERNAME, API_KEY)
    
    print(f"\nüîç URL: {search_url}")
    print(f"üåç Proxy: DE (Germany)")
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            print("\n‚è≥ Sending request to ScrapingBot...")
            
            response = await client.post(
                endpoint,
                json=payload,
                headers=headers,
                auth=auth
            )
            
            print(f"\n‚úÖ Response Status: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                
                if result.get("error"):
                    print(f"‚ùå Error: {result['error']}")
                    return
                
                html = result.get("rawHtml", "")
                
                if not html:
                    print("‚ùå No HTML content")
                    return
                
                # Check for CAPTCHA/blocking
                if "captcha" in html.lower() or "just a moment" in html.lower():
                    print("‚ùå CAPTCHA/blocking detected")
                    return
                
                print(f"‚úÖ Success! HTML Length: {len(html):,} characters")
                
                # Parse HTML
                soup = BeautifulSoup(html, 'lxml')
                
                # Look for products
                products = soup.find_all(['div', 'article'], class_=lambda x: x and ('product' in str(x).lower() or 'item' in str(x).lower()))
                
                print(f"\nüîç Found {len(products)} potential product containers")
                
                # Look for prices (‚Ç¨ symbol)
                price_elements = soup.find_all(string=re.compile(r'‚Ç¨|EUR'))
                print(f"üí∞ Found {len(price_elements)} price elements")
                
                if price_elements:
                    print(f"\nüìå Sample prices:")
                    for i, price in enumerate(price_elements[:5], 1):
                        print(f"   {i}. {price.strip()}")
                
                # Look for product links
                links = soup.find_all('a', href=True)
                product_links = []
                for link in links:
                    href = link.get('href', '')
                    # HHV product URLs typically contain /shop/
                    if '/shop/' in href or '/product/' in href:
                        if href.startswith('/'):
                            href = f"https://www.hhv.de{href}"
                        product_links.append(href)
                
                print(f"\nüîó Found {len(product_links)} product links")
                
                if product_links:
                    print(f"\nüìå First 3 product links:")
                    for i, link in enumerate(product_links[:3], 1):
                        print(f"   {i}. {link}")
                    
                    print(f"\nüéâ SUCCESS! HHV can be scraped!")
                    print(f"‚úÖ No CAPTCHA, products and prices found")
                else:
                    print("\n‚ö†Ô∏è  No obvious product links found")
                    # Show some links to debug
                    all_links = [l.get('href') for l in links[:10]]
                    print(f"Sample links: {all_links}")
                    
            else:
                print(f"‚ùå Error: HTTP {response.status_code}")
                print(f"Response: {response.text[:500]}")
                
    except Exception as e:
        print(f"\n‚ùå Exception: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 70)

if __name__ == "__main__":
    asyncio.run(test_hhv())
